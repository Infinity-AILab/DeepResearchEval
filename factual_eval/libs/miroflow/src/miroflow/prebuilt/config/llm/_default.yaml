# conf/llm/default.yaml - Default LLM configuration
# provider: "claude_openrouter" # openai, qwen, anthropic, gemini, deepseek
# model_name: "google/gemini-2.5-flash" # gpt-4.1, qwen3-14b, claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, gemini-1.5-pro, deepseek-chat
provider: "openai" # openai, qwen, anthropic, gemini, deepseek
model_name: "gpt-5-mini" 
async_client: true
temperature: 1.0
top_p: 1.0
min_p: 0.0
top_k: -1
max_tokens: 128000
max_context_length: 400000  # Maximum context length for the model
openai_base_url: null
openrouter_provider: null  # For OpenRouter: set to "google", "amazon", etc. to force specific provider
disable_cache_control: false  # Set to true to disable cache control for Anthropic models
keep_tool_result: -1
anthropic_base_url: https://api.anthropic.com
oai_tool_thinking: false 

{
  "url": "https://gemini.google.com/app/ae2f5b27d4643e59",
  "title": "Deep Research 2: Science & Technology",
  "timestamp": "2025-08-19T23:33:23.475871",
  "entries": [
    {
      "timestamp": "2025-08-19T23:33:23.473522",
      "query": "Between January 2024 and August 2025, how have AI-generated visuals affected public trust in science communication across US, EU, and China? Provide case studies, policy responses, sentiment metrics, and recommendations.",
      "response": "Conversation with Gemini   Between January 2024 and August 2025, how have AI-generated visuals affected public trust in science communication across US, EU, and China? Provide case studies, policy responses, sentiment metrics, and recommendations.  Here's the plan I've put together. Let me know if you need any changes before I start researching.\n  AI Visuals' Impact on Science Trust  Research Websites (1) Find documented case studies and notable examples from January 2024 to August 2025 where AI-generated visuals were used in science communication in the US, EU, and China, and analyze their impact on public discourse and perception.\n(2) Research academic studies, think tank reports, and journalistic analyses that evaluate how the use of AI visuals has specifically affected public trust in scientific institutions and communicators within these regions and the specified timeframe.\n(3) Identify and describe any sentiment metrics, public opinion polls, or survey data that quantify shifts in public trust related to science communication that employs AI-generated content.\n(4) Investigate policy responses, ethical guidelines, and regulatory frameworks developed by governmental bodies, scientific organizations, and technology platforms in the US, EU, and China to address the use of AI visuals in science.\n(5) Compare and contrast the nature of the challenges, public reactions, and policy approaches regarding AI in science communication across the US, EU, and China.\n(6) Synthesize the findings to develop a set of actionable recommendations for key stakeholders, including:\n   (a) Scientists and communicators for the ethical and effective use of AI tools.\n   (b) Policymakers for creating robust governance frameworks.\n   (c) Educators and the public for enhancing media and AI literacy.\n(7) Explore expert forecasts and speculative analyses on the future evolution of AI-generated visuals in science and the potential long-term consequences for public understanding and trust. More Analyze Results Create Report Ready in a few mins  Edit plan   Start research   Start research  I've completed your research. Feel free to ask me follow-up questions or request changes. AI Visuals' Impact on Science Trust Completed Aug 19, 11:33 PM Deep Research  Gemini can make mistakes, so double-check it  AI Visuals' Impact on Science Trust Contents Export Create The Synthetic Scientist: AI Visuals and the Shifting Sands of Public Trust in Science (2024-2025) Section 1: Introduction: The Double-Edged Scalpel Between January 2024 and August 2025, the proliferation of generative artificial intelligence (AI) for visual content creation marked a profound inflection point for science communication. This period witnessed the technology’s transition from a niche novelty to a pervasive, mainstream force, fundamentally altering the production, dissemination, and public consumption of scientific information.  [1](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)   The rise of sophisticated, accessible tools capable of generating photorealistic images and video from simple text prompts has presented the scientific community with a double-edged scalpel: a powerful instrument with unprecedented potential to clarify complex concepts and engage the public, yet equally capable of inflicting deep wounds on the very fabric of public trust.     The unique potency of the visual medium lies at the heart of this challenge. Humans process visual information significantly faster and with greater emotional resonance than text, making well-designed visuals an invaluable asset for translating complex scientific principles into comprehensible insights.  [3]   AI-powered tools can create schematic diagrams, animate cellular processes, and visualize abstract data with an efficiency and scale previously unimaginable, promising to democratize scientific understanding.  [4]   However, this same immediacy makes AI-generated visuals a formidable vector for deception. Research indicates that the average person often cannot distinguish between authentic and synthetic content, a vulnerability that malicious actors can exploit to propagate sophisticated misinformation.  [6]   This dynamic fuels the \"liar's dividend,\" a corrosive societal effect where the mere possibility of a convincing fake allows genuine evidence—from satellite imagery of climate change to micrographs of viral structures—to be dismissed as synthetic.     The challenge is compounded by the emergence of what has been termed \"AI slop\": a deluge of low-quality, often nonsensical AI-generated media that clutters the digital ecosystem.  [7]   While not always intentionally malicious, this digital detritus creates a background noise of unreality that exhausts the public's cognitive resources. The constant exposure to both low-grade \"slop,\" such as the viral \"Shrimp Jesus\" images on Facebook, and high-threat disinformation, like deepfake doctors promoting fraudulent cures, risks pushing audiences past a threshold of active skepticism into a state of \"reality apathy\".  [7]   In this state, the cognitive burden of verifying every image becomes overwhelming, leading to a blanket distrust of all visual information. This disengagement poses a catastrophic threat to evidence-based communication, particularly in critical domains like public health and climate science, where visual evidence is often essential for conveying urgency and motivating action.     This report provides an exhaustive analysis of how AI-generated visuals affected public trust in science communication across three distinct geopolitical and regulatory ecosystems: the United States, the European Union, and China. It examines the divergent paths these regions have taken—a market-driven and fragmented approach in the US, a rights-based regulatory framework in the EU, and a state-controlled, stability-focused model in China—to navigate this new technological frontier.  [2](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)   Through an analysis of case studies, policy responses, and public sentiment metrics from January 2024 to August 2025, this report illuminates the complex interplay between technology, governance, and public trust, concluding with strategic recommendations for key stakeholders.     Section 2: The New Visual Vernacular: Technology, Adoption, and Research 2.1 The Cambrian Explosion of Visual AI (2024-2025) The period between early 2024 and mid-2025 was characterized by a veritable Cambrian explosion in the capabilities and accessibility of generative visual AI. Text-to-image and text-to-video models, once the domain of specialized research labs, matured into robust, widely available tools. Companies like Google (with Imagen and Veo), OpenAI (DALL-E and Sora), and Midjourney released models that demonstrated dramatic improvements in realism, coherence, and prompt adherence.  [9]   This rapid advancement was quantifiable; performance on demanding new benchmarks like MMMU and GPQA, introduced in 2023 to test the limits of advanced AI, saw scores rise by 18.8 and 48.9 percentage points, respectively, within a single year.  [12]      This technological leap was driven overwhelmingly by industry, which produced nearly 90% of notable AI models in 2024, a significant increase from 60% in 2023.  [13]   This industry dominance led to a swift integration of generative AI features into mainstream consumer and professional platforms. Canva, for instance, integrated its \"Magic\" suite of AI tools, powered by models like GPT-3.5, allowing its vast user base of non-designers to generate high-quality visuals and designs instantly. Within months of launch, over 1 billion AI-powered actions were performed on the platform, signaling a profound democratization of sophisticated content creation.  [10]   This shift meant that the ability to generate compelling, and potentially misleading, visuals was no longer confined to experts but was now in the hands of millions.     2.2 Adoption in Science Communication The scientific community and its adjacent communicators were quick to explore the potential of this new visual vernacular, leading to a spectrum of applications ranging from the highly beneficial to the deeply problematic. On the positive end of the spectrum, these tools were adopted to enhance the accessibility and comprehension of complex scientific topics. Researchers and educators began using AI to generate clear schematic representations of cellular processes, conceptual diagrams of theoretical frameworks, and visually engaging abstracts to summarize key findings.  [4]   In fields like medical education, AI offered significant speed and cost advantages over hiring traditional illustrators, with the potential to create vast libraries of customizable anatomical images for training purposes.  [15]      However, the peril of this widespread adoption became equally apparent. The same technologies were co-opted for deceptive purposes, blurring the lines between authentic and synthetic science. Online retailers began using AI-generated images of fantastical, non-existent plants to sell seeds to unsuspecting gardeners.  [7]   More alarmingly, studies evaluating AI's ability to generate anatomical illustrations found that while the tools were fast and cheap, they often produced inaccurate and misleading images, omitting critical details or misrepresenting key structures.  [15]   This highlighted a critical risk: the potential for a flood of visually plausible but scientifically flawed content to pollute educational and public information channels.     2.3 The State of Academic Inquiry The academic community's response to this technological disruption has been robust but is still in its early stages. A literature review of leading science communication journals reveals that research on the intersection of AI and science communication is a nascent but rapidly growing field.  [1](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)   While a handful of studies existed before 2020, the field saw a dramatic surge in interest following the public release of powerful generative tools, with over 40% of all related articles in top journals being published in 2024 alone.  [1](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)      This emerging body of research, however, has notable limitations. The primary focus has been on analyzing public perceptions of AI, with a significant gap in understanding how science communicators themselves are adopting, adapting to, and being shaped by these new tools.  [4]   There is a lack of in-depth exploration into the systemic impacts of AI on the broader science communication ecosystem. Furthermore, the research has been predominantly concentrated in Western contexts, particularly the US and UK, with a methodological inclination toward quantitative surveys.  [1](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)   This leaves a critical void in understanding these dynamics in different cultural and regulatory environments, underscoring the need for more geographically and methodologically diverse research to fully grasp the global implications of this transformation.     Section 3: A Tale of Three Ecosystems: Regional Analysis of Impact and Response The global landscape of AI adoption and its impact on public trust is not monolithic. The United States, the European Union, and China have forged distinct paths shaped by their unique political philosophies, regulatory appetites, and cultural contexts. The following tables provide a quantitative and qualitative framework for understanding these divergent ecosystems, summarizing key metrics on public sentiment and the core tenets of their respective regulatory approaches. These frameworks set the stage for a detailed regional analysis of case studies and policy responses. Metric United States European Union China Source Snippets Overall AI Optimism (% viewing AI as more beneficial than harmful) 39% (with a 4% increase since 2022) Varies (e.g., Germany 47%, France 41%, Netherlands 36%) 83%  [12]  Trust in Scientific Research Using AI N/A (General distrust is high) 38% trust vs. 25% distrust N/A (Implied high trust)  [18]  Concern vs. Excitement about AI in Daily Life 52% more concerned vs. 10% more excited N/A N/A (Implied high excitement)  [19]  Expert vs. Public Optimism Gap Experts: 56% positive impact vs. Public: 17% N/A N/A  [21]  Concern about AI-driven Misinformation 83.4% concerned about election misinformation High (WEF global risk) High (Govt. justification for regulation)  [6]      Regulatory Pillar United States European Union China Source Snippets Core Legislation No single federal law; reliance on existing FTC rules, executive orders. \"America's AI Action Plan\" proposes deregulation. EU AI Act (enforced from Aug 2024) Deep Synthesis Regulation (Jan 2023), Generative AI Regulation (Aug 2023), New Labeling Rules (Sep 2025)  [23]  Labeling/Watermarking No federal mandate; platform-dependent (e.g., Meta's disclosure tool). Mandatory:  AI outputs must be marked as artificially generated in a machine-readable format. Mandatory & Strict:  All AI content must have visible tags/watermarks. Implicit metadata also required.  [24]  Consent for Biometrics No federal mandate; varies by state law. Required under GDPR. Mandatory & Explicit:  Written consent required to use a person's face or voice.  [25]  Content Moderation Platform discretion, protected by Section 230. \"AI Action Plan\" seeks to limit moderation on ideological grounds. AI Act prohibits certain high-risk applications. Focus on transparency rather than content bans. Strict Prohibitions:  Banned use for \"fake news,\" political manipulation, scams. Real-name registration required for users.  [23]  Governing Philosophy Promote innovation, free market, free speech. Protect fundamental rights, ensure safety and transparency (human-centric). Ensure social stability, state control, and national security.  [23]      3.1 The United States: Innovation, Skepticism, and a Fractured Trust Landscape The American experience with AI in science communication is defined by a deep and widening chasm between expert optimism and public skepticism. This \"enthusiasm gap\" creates a fertile ground for distrust, where the rapid, market-driven deployment of AI technologies outpaces the development of social and regulatory guardrails. Public Sentiment: \nPolling data from 2024 and 2025 paints a stark picture of a wary American public. A Pew Research Center study found that only 17% of U.S. adults believe AI will have a positive impact on the country over the next 20 years, compared to 56% of AI experts.  [21]   This disconnect is particularly acute regarding the technology's impact on work, where 73% of experts foresee positive effects, a view shared by only 23% of the public.  [22]   Overall, 52% of Americans report being more concerned than excited about the increased use of AI in daily life, with only 10% feeling the opposite.  [19]   This generalized anxiety translates directly into distrust of AI in information contexts. A study by Poynter and the University of Minnesota found that nearly half of Americans do not want news from generative AI, and more than half have little to no confidence in newsrooms using AI to create photos or articles.  [34]   The demand for transparency is overwhelming, with over 90% of engaged news consumers wanting clear disclosure when AI is used.  [34]      Case Studies: \nThis fractured trust landscape is reflected in the divergent applications of AI visuals in the U.S. Positive Application (Public Health):  A pioneering study from the University of Pennsylvania's Leonard Davis Institute of Health Economics (LDI) demonstrated AI's potential for highly effective public health communication. Researchers developed a \"Living Method\" that used AI to analyze social media conversations and identify resonant messages for HIV prevention among Men who have Sex with Men (MSM). The AI-selected messages were six times more likely to be adopted and posted by public health agencies in 42 U.S. jurisdictions than traditionally sourced content, showcasing AI's ability to create targeted, community-based campaigns at scale.  [35]   Similarly, the U.S. Centers for Disease Control and Prevention (CDC) adopted a cautious policy of using generative AI to assist in creating public-facing graphics, but only with a mandatory review by human experts to ensure accuracy, a practice designed to build and maintain trust.  [36]      Negative Application (Medical Misinformation):  A far more damaging narrative emerged with the proliferation of \"deepfake doctors\" across social media platforms in 2024 and 2025. A CBS News investigation uncovered dozens of accounts using AI-generated videos, some impersonating real physicians, to promote unproven and potentially dangerous wellness products with claims like being \"96% more effective than Ozempic\".  [8]   These incidents, some of which garnered millions of views, directly attacked the credibility of medical professionals. As one impersonated doctor noted, this content \"distorts what fact is, and makes it harder for the public to believe anything that comes out of science\".  [8]   This case study exemplifies the direct harm to public health and trust when AI visuals are weaponized to exploit the authority of science.     Politicized Science:  The use of AI visuals to manipulate public discourse around scientific issues became increasingly common. During the 2024 U.S. presidential election, AI-generated content was a major source of public concern, with 83.4% of Americans expressing worry about its role in spreading misinformation.  [6]   Specific incidents, such as the circulation of a fake AI-generated image of a girl in a flood after Hurricane Helene to criticize the government's disaster response, demonstrated how easily scientific realities like climate-driven weather events can be co-opted for political narratives, further eroding trust in evidence-based discussions.  [7]      Policy & Governance: \nThe U.S. policy response is characterized by fragmentation and a deep ideological divide, creating an unstable and unpredictable regulatory environment. The Biden administration, through its 2023 AI Executive Order and the State Department's Enterprise AI Strategy, has emphasized a \"responsible AI\" framework focused on safety, security, and countering disinformation.  [37]   However, this approach contrasts sharply with the \"America's AI Action Plan\" proposed by the Trump administration, which champions a deregulatory agenda aimed at securing U.S. dominance in AI by removing regulatory barriers.  [23]   This plan explicitly calls for prioritizing innovation and free speech, going so far as to recommend that the National Institute of Standards and Technology (NIST) revise its AI Risk Management Framework to remove references to misinformation.  [23]   This fundamental disagreement on whether to regulate AI-driven misinformation as a harm to be mitigated or protect it as a form of speech leaves the American public without a clear, consistent set of rules governing the new information ecosystem.     3.2 The European Union: A Rights-Based Approach to Building Trust In contrast to the market-driven approach of the U.S., the European Union has pursued a proactive, rights-based regulatory strategy designed to build public trust by embedding safeguards directly into the technology's legal framework. This approach is rooted in a public sentiment that is generally supportive of science but demands clear governance for powerful new technologies. Public Sentiment: \nEurobarometer survey data indicates that a vast majority of EU citizens (83%) believe the overall influence of science and technology is positive.  [18]   However, this optimism is tempered with caution when it comes to AI. A 2025 survey revealed mixed views on AI's role in scientific research, with 38% of citizens expressing trust, while a significant minority of 25% expressed distrust.  [18]   This data suggests a public that is not anti-technology but is looking for clear rules and accountability, providing a strong political mandate for the EU's comprehensive regulatory efforts.     Case Studies: \nThe European approach has fostered applications of AI visuals that emphasize public engagement, education, and the development of trustworthy systems. Public Engagement (Museums & Culture):  European cultural institutions have been at the forefront of using AI to foster a more critical and engaging public dialogue about science and technology. In Dublin, the ADAPT Research Centre's #ExploreAI exhibit, part of the European Researchers' Night 2024, invited the public to interact directly with AI-generated art and virtual reality systems to question the nature of creativity and perception.  [38]   In Paris, the Jeu de Paume museum's \"The World Through AI\" exhibition showcased artists using AI in a reflexive, critical manner, aiming to move the public from being passive consumers to active, informed participants in the AI revolution.  [39]   These initiatives use AI not merely as a tool for dissemination but as a subject of public inquiry, aiming to build a more resilient and informed citizenry.     Public Health (Frameworks & Infrastructure):  The EU's strategy extends beyond individual applications to building institutional capacity for trustworthy AI. A key example is the establishment of TEF-Health (Testing and Experimentation Facility for AI in Health), a €60 million initiative launched in January 2023.  [40]   TEF-Health provides a sandbox environment where AI developers can test their health-related AI solutions against the rigorous standards of the AI Act, ensuring that products reaching the market are safe, reliable, and ethically sound. This proactive, infrastructure-based approach aims to build trust not as an afterthought but as a foundational component of the innovation lifecycle.  [28]      Policy & Governance: \nThe cornerstone of the EU's strategy is the Artificial Intelligence Act, a landmark piece of legislation that entered into force in August 2024 and whose obligations began to apply in 2025.  [28]   This report's analysis focuses on Article 50, which establishes clear transparency obligations for AI-generated content. The Act mandates that providers of AI systems generating synthetic content must ensure their outputs are \"marked in a machine-readable format and detectable as artificially generated or manipulated\".  [24]   Furthermore, deployers of \"deepfakes\"—content that appears to be authentic but is not—are required to disclose that the content has been artificially generated. While there are limited exceptions for clearly artistic or satirical works, the default principle is transparency.  [41]   This \"transparency-by-design\" philosophy is the EU's primary mechanism for combating misinformation and building a foundation of public trust, making it clear to citizens when they are interacting with synthetic media.     3.3 China: State-Led Adoption and Top-Down Trust Management China presents a third, distinct model, where the development and deployment of AI are tightly interwoven with national strategic goals, and public trust is managed through a top-down system of stringent control and state-led promotion. Public Sentiment: \nPublic opinion data from China reveals a level of optimism about AI that is dramatically higher than in the West. The 2025 Stanford HAI AI Index Report found that 83% of Chinese respondents believe AI products and services offer more benefits than drawbacks, compared to just 39% in the United States.  [12]   This widespread optimism is not accidental; it is cultivated within a state-controlled information environment that consistently frames technological advancement as a key pillar of national strength and economic prosperity, while simultaneously censoring criticism and highlighting the potential chaos of unregulated Western tech ecosystems.  [32]      Case Studies: \nChina's use of AI visuals in science communication reflects this state-led approach, emphasizing large-scale public education and rapid, government-guided adoption. Public Education (National Science Museums):  The official opening of the new Shenzhen Science & Technology Museum in May 2025 serves as a powerful case study in state-led technological showcases. Designed by Zaha Hadid Architects and dubbed the \"starship of the future,\" the museum integrates AI into its core visitor experience.  [44]   Exhibits like the \"Hello, World!\" welcome hall use interactive AI avatars and gesture-controlled displays to make complex concepts like blockchain and big data accessible, while the \"DREAM SPACE\" uses real-time XR and motion synthesis to immerse visitors in digital scientific scenes.  [44]   These exhibits present AI not just as an educational tool but as a symbol of China's technological prowess and future-oriented vision.     Scientific Research & Application:  This top-down push is also evident in scientific research. A 2025 European Commission analysis found that Chinese scientists have incorporated AI into their work more rapidly and effectively than their US or EU counterparts, leading in both the sheer output and the scientific novelty of AI-assisted research papers.  [46]   The development of powerful domestic models, such as Baidu's ERNIE, is a national priority. These models are promoted for applications ranging from academic research summarization to powering national search engines, ensuring the AI ecosystem develops within the government's technological and ideological boundaries.  [47]      Content Regulation (Public Health):  When the benefits of AI are threatened by misuse, the state's response is swift and decisive. In August 2025, facing a rise in online medical misinformation, several Chinese regulators, including the National Health Commission, jointly issued new rules. These regulations mandate strict verification of medical credentials for anyone creating online health content, require explicit disclosure of AI-generated material, and completely ban the inclusion of advertisements or product links.  [50]   This rapid, top-down intervention to sanitize the information ecosystem demonstrates the government's commitment to maintaining public trust in state-approved health narratives.     Policy & Governance: \nChina's regulatory framework for generative AI is the most prescriptive and control-oriented in the world. The Administrative Provisions on Deep Synthesis in Internet-Based Information Services (effective January 2023) and the new Measures for Labeling Artificial Intelligence-Generated Content (effective September 2025) form a comprehensive system of oversight.  [25]   Key provisions include a mandatory requirement for all AI-generated content to be conspicuously marked with a visible watermark or tag, such as \"AI-generated\".  [25]   The regulations also require service providers to implement real-name registration for all users of deep synthesis tools and explicitly ban the use of the technology for spreading \"fake news,\" engaging in political manipulation, or impersonating individuals without their explicit consent.  [25]   This framework prioritizes social stability and state control above all else, creating a tightly managed information environment where public trust is a function of state-enforced order.     The starkly different outcomes in these three regions reveal a compelling dynamic regarding governance and public confidence. The data suggests a paradox: the region with the most stringent, top-down controls on AI-generated content (China) reports the highest levels of public trust and optimism, while the region with the most fragmented and hands-off approach (the United States) exhibits the lowest. The European Union, with its balanced, rights-focused regulatory framework, occupies a middle ground in public sentiment. This pattern challenges the conventional Silicon Valley argument that regulation inherently stifles public acceptance. For a technology as potent and potentially reality-distorting as generative AI, the evidence from 2024-2025 suggests that a visible governance framework, even a highly restrictive one, may be more conducive to public trust than a perceived regulatory vacuum. The public's deep-seated concern about misinformation and manipulation appears to create a demand for guardrails. In the absence of clear rules, as seen in the U.S., public skepticism flourishes. The state's visible hand in China, promising to curb the technology's worst excesses, appears to provide a level of reassurance that fosters public optimism, albeit at the cost of freedoms valued elsewhere. The EU's model, therefore, emerges as a potential template for democratic societies seeking to build trust through transparent regulation without resorting to authoritarian control. Section 4: The Gatekeepers: Institutional Responses and Emerging Norms Beyond national governments, a critical layer of governance is emerging from the institutions that act as gatekeepers of scientific and public information: scientific publishers and major technology platforms. Their policies, developed in real-time throughout 2024 and 2025, are shaping the norms for the responsible use of AI-generated visuals. 4.1 The Scientific Publishing Community: Drawing a Line in the Sand The guardians of scientific integrity—the world's leading academic journals—responded to the challenge of AI-generated visuals with a mixture of outright prohibition and demands for radical transparency. Faced with a technology that could fabricate data and undermine the bedrock of verifiable evidence, publishers moved to establish clear and strict policies. A consensus quickly formed around a few core principles. First, an AI or large language model cannot be credited as an author, as authorship entails accountability and responsibility that a machine cannot assume.  [53]   Second, the ultimate responsibility for the integrity and accuracy of all content, including any AI-assisted elements, rests solely with the human authors.  [53]      On the specific issue of AI-generated images, a split emerged. A cohort of the most influential journals adopted a hardline stance.  Nature  and its parent company Springer Nature announced they would not allow the inclusion of generative AI images in their publications, citing fundamental concerns about scientific integrity, transparency, and the inability of current tools to provide verifiable sources for their outputs.  [56]   The publisher Elsevier adopted a similar policy, not permitting the use of generative AI to create or alter images in submitted manuscripts.  [54]   The      Science  family of journals likewise banned AI-generated figures and images unless explicit permission was granted by the editors, stating that a violation would constitute scientific misconduct.  [59]      Other major publishers forged a path based on rigorous disclosure. The American Chemical Society (ACS) and the Institute of Electrical and Electronics Engineers (IEEE) permit the use of AI-generated visuals for illustrative purposes, such as schematics or journal cover art, but only with exhaustive documentation.  [53]   Their policies require authors to disclose the use of AI tools in the acknowledgments or methods section, provide a description of how the tools were used, and include a brief explanation in the figure caption itself. The ACS policy notably prohibits AI-generated images in Table of Contents graphics, as they lack the space for an immediately adjacent disclosure.  [60]   This approach seeks to balance the potential utility of the tools with the non-negotiable requirement for transparency and reproducibility.     The following table summarizes the policies of major scientific publishers as they stood in mid-2025, illustrating the emerging industry standards. Publisher / Journal Policy on AI-Generated Visuals in Papers Key Requirements Source Snippets Nature / Springer Nature Banned.  Not allowed for inclusion in publications. Confirmation required that no submitted works were created/enhanced with generative AI.  [56]  Science Journals Banned, unless explicit permission from editors is granted. Violation constitutes scientific misconduct.  [59]  Elsevier Not permitted  to create or alter images. Adjustments to brightness/contrast are acceptable if they don't obscure information. Original images may be requested.  [54]  PLOS Discouraged  for critical research components. If used, must be clearly identified with exhaustive documentation. Must not contain fabricated data.  [53]  IEEE Permitted with disclosure. Must be clearly identified, system must be credited, author must verify no copyright violation.  [53]  ACS Publications Permitted with disclosure.  (Not for Table of Contents graphics). Disclosure in Acknowledgments/Methods. Brief description in figure caption.  [60]      4.2 The Platform Providers: Divergent Philosophies of Moderation The technology platforms where AI-generated content is created and disseminated have adopted policies that reflect their underlying business models and the regulatory environments in which they operate. Western Platforms (Google, Meta): \nIn the U.S. and EU, major platforms like Google and Meta navigate a complex balancing act between promoting user safety, protecting freedom of expression, and maximizing engagement. Their policies are often principles-based and enforced reactively. Google's Prohibited Use Policy for generative AI forbids the creation of content that facilitates misleading claims in sensitive areas like health and civic processes \"in order to deceive\".  [61]   However, it carves out explicit exceptions for \"educational, documentary, scientific, or artistic considerations,\" creating a significant gray area.  [61]   In its core Search product, Google's ranking systems prioritize content that demonstrates expertise, experience, authoritativeness, and trustworthiness (E-E-A-T), focusing on the quality of the information rather than its method of production.  [62]      Meta's approach relies heavily on user disclosure. The company requires users to label photorealistic video or realistic-sounding audio created with AI using a dedicated tool, and may apply penalties for failure to do so.  [27]   However, its misinformation policy primarily triggers content removal only when the content is likely to contribute to \"imminent physical harm\" or interfere with political processes.  [27]   This high threshold leaves vast quantities of misleading but not immediately dangerous scientific or health content to circulate freely, often with only a label or reduced distribution as a consequence.     Chinese Platforms (Tencent, Baidu): \nIn China, technology platforms function as extensions of the state's regulatory apparatus. Their policies are not a matter of philosophical debate but of direct compliance with government mandates. Companies like Tencent and Baidu operate under the strictures of the Deep Synthesis Regulation and other state rules.  [25]   Tencent Cloud, for example, markets its Media Content Moderation (MCM) service to clients as a solution to ensure compliance by detecting violations like deepfakes, explicit content, and misinformation.  [63]   This aligns the company's commercial interests with the state's goal of information control. Similarly, Baidu actively promotes its ERNIE model for state-sanctioned applications like academic research and education, ensuring that its powerful generative capabilities are deployed in ways that support national objectives.  [47]   The primary role of these platforms is not to moderate a debate but to enforce a set of clear, top-down rules.     Section 5: Synthesis: Cross-Cutting Insights and the Trust Paradox Across the diverse applications and regional responses, several cross-cutting themes emerge that reveal the nuanced and often counterintuitive impact of AI-generated visuals on public trust in science. These insights challenge simple narratives of \"good\" versus \"bad\" technology, highlighting the complex psychological and social dynamics at play. 5.1 The Trust Paradox Revisited: When Clarity Trumps Credibility A foundational assumption in science communication is that trust flows from the credibility of the source. However, research from this period suggests a troubling paradox. A 2024 study published in  PNAS Nexus  compared lay readers' reactions to scientific summaries written by scientists versus those generated by AI. The findings were startling: participants perceived the AI-generated summaries as  more trustworthy  and demonstrated better recall of the information they contained.  [4]   The AI's ability to produce clear, simple, and confidently-worded text appeared to be more persuasive to a non-expert audience than the more cautious and nuanced language typically employed by human scientists. This creates a dangerous dynamic where the      perception  of trustworthiness can become decoupled from actual accuracy. A \"hallucinating\" AI that confidently presents a falsehood in simple terms may be trusted more than a human expert who accurately describes the complexities and uncertainties inherent in the scientific process. This paradox is a critical vulnerability that can be exploited to spread misinformation under a veneer of AI-generated clarity. 5.2 The Asymmetry of Trust Erosion Public trust is a fragile commodity, far easier to destroy than to build. The events of 2024-2025 demonstrate a powerful asymmetry in the impact of AI-generated visuals. A single, viral case of a deepfake doctor promoting a fake cure or a fabricated image of a climate disaster used for political gain can inflict widespread and lasting damage on public trust in scientific and medical institutions.  [7]   The emotional impact of being deceived creates a powerful and memorable negative association. In contrast, the positive, trust-building applications of AI—such as an engaging museum exhibit or an effective public health campaign—have a more diffuse and less dramatic impact.  [35]   This asymmetry means that a purely reactive approach to content moderation is destined to fail; the damage is often done before a piece of misinformation can be identified and removed. It underscores the necessity of proactive, preventative measures, such as robust watermarking and strict transparency mandates, to inoculate the public against deception before it occurs.     5.3 The \"Uncanny Valley\" of Scientific Avatars The use of AI-generated avatars as science communicators introduces further complexities. The \"uncanny valley\" hypothesis has long suggested that highly realistic but imperfect human replicas evoke feelings of unease and distrust.  [65]   However, a 2025 experiment published in the      Journal of Science Communication  directly contradicted this theory in the context of science communication. The study found that audiences perceived more realistic AI avatars as significantly  more  trustworthy—rating them higher on expertise, integrity, and benevolence—than their more stylized, cartoonish counterparts.  [2](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)   This suggests that for conveying credible information, realism may be an asset rather than a liability. However, the same study revealed the persistence of human biases within these synthetic representations. Participants rated male avatars as having higher expertise than female avatars, even when they delivered identical information.  [2](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)   This finding highlights how AI systems, trained on vast datasets of existing human interactions and media, can inadvertently absorb and amplify societal biases. This complicates the role of AI as a neutral conduit for science, demonstrating that even the appearance of a synthetic communicator is laden with social cues that can influence public trust and perception.     5.4 The Hidden Environmental Cost Finally, a critical and often-overlooked factor shaping the ethical landscape of AI-generated visuals is their substantial environmental footprint. The immense computational power required to train and deploy large-scale generative models like GPT-4, DALL-E, and Imagen translates into staggering demands for electricity and water.  [66]   Data centers running these models contribute to increased carbon dioxide emissions and place significant strain on local power grids and water supplies.  [66]   A 2024 MIT paper on the climate implications of generative AI highlighted these resource-intensive processes as a major sustainability concern.  [66]   This introduces a new and profound ethical dimension to science communication. Is the communicative benefit of generating a novel AI visual for a presentation or a social media post worth its environmental cost? As awareness of this \"hidden tax\" on the planet grows, it could become a new source of public skepticism, particularly in communications related to environmental science, where the hypocrisy of using an energy-intensive tool to advocate for sustainability would be stark.     Section 6: Future Outlook and Strategic Recommendations The period from 2024 to 2025 was a crucible, forging a new and volatile relationship between AI, science communication, and public trust. Looking ahead, the trajectory of this relationship will be determined by the strategic choices made by policymakers, scientific institutions, communicators, and technology developers. Based on the analysis of current trends and expert forecasts, this section outlines the likely future landscape and provides actionable recommendations for navigating its challenges and opportunities. 6.1 Forecasts and Future Trajectories (2025-2035) Expert forecasts point toward an acceleration of the trends observed in this report, leading to a future where AI is not just a tool but a fundamental part of the information environment. Technological Trajectory:  The next decade is expected to see the maturation of  multimodal AI, where systems seamlessly integrate and process text, voice, images, and video to create hyper-personalized and intuitive communicative experiences.  [68]   The      democratization of AI  will continue, with user-friendly, no-code platforms making the creation of custom AI solutions accessible to individuals and small organizations without deep technical expertise.  [68]   This will further increase the volume and sophistication of synthetic media in circulation.     Societal Trajectory:  The societal impact of this transformation remains a subject of intense debate among experts. A 2024 survey revealed a stark division: over 50% of experts expect AI's net impact by 2040 to be negative for privacy, political stability, and human rights, yet a similar majority expects a net positive impact on healthcare, scientific research, and economic productivity.  [69]   The timeline for the arrival of transformative or artificial general intelligence (AGI) is highly uncertain, but a significant portion of experts believe its development is plausible within the next several decades, a timeframe that necessitates urgent consideration of its societal implications.  [70]      6.2 Recommendations for Stakeholders To navigate this complex future and rebuild a foundation of trust in science, a concerted, multi-stakeholder effort is required. The following recommendations are tailored to the key actors shaping the AI and science communication ecosystem. For Policymakers & Regulators (US, EU, and Global Bodies) Adopt a \"Transparency by Design\" Mandate:  The EU AI Act provides a valuable model. Governments, particularly in the U.S., should move beyond reliance on voluntary platform policies and establish a clear legal mandate for the identification of synthetic media. This should involve supporting the development and adoption of open technical standards for digital watermarking and content provenance, creating a reliable, machine-readable baseline of authenticity for all digital content. Fund Public Digital Literacy at Scale:  The significant gap between public perception of AI use and the reality, as identified in studies by Poynter and Pew, represents a critical vulnerability.  [34]   Governments should fund large-scale national education campaigns aimed at improving digital literacy. These programs should focus on teaching citizens how to identify the hallmarks of synthetic media and understand the mechanisms of algorithmic persuasion, empowering them to be more critical consumers of information.     Invest in \"Pro-Truth\" AI Research:  Public funding should be directed toward research and development of AI systems designed to bolster the integrity of the information ecosystem. This includes sponsoring the creation of advanced AI tools for detecting sophisticated manipulations, verifying the sources of visual claims in real-time, and accurately summarizing the state of scientific consensus on complex topics, thereby using AI to counter its own potential for misuse. For Scientific Institutions & Funding Bodies (e.g., Royal Society, National Academies) Develop Clear Institutional AI Usage Policies:  Every research university, scientific society, and funding agency must follow the lead of organizations like the Royal Society and establish clear, public-facing guidelines on the ethical and responsible use of generative AI by their researchers, staff, and communicators.  [71]   These policies should cover data integrity, authorship, disclosure, and the potential for bias.     Mandate Reproducibility for AI-Assisted Research:  To counter the \"black box\" problem of some AI models, funding bodies should make reproducibility a condition of grants for research that heavily utilizes AI. This should include requiring applicants to submit reproducibility checklists, data and model sharing protocols, and access to the code used in their analysis or visualization, thereby upholding the principles of open science.  [71]      Incentivize Interdisciplinary Collaboration:  The most effective and ethical AI tools are built with diverse expertise. Funders and institutions should create programs that incentivize meaningful collaboration between AI developers, domain scientists (e.g., climatologists, biologists), and social scientists or ethicists to ensure that AI tools are developed and applied in a socially robust, accurate, and ethical manner.  [71]      For Science Communicators & Journalists Practice \"Radical Transparency\":  Given the high levels of public skepticism, the default posture must be one of complete transparency. When using AI to generate or assist with visuals, communicators must always disclose this fact clearly and prominently. This disclosure should ideally include the tool used, the prompt given, and a brief description of the extent of human review and editing. This practice builds credibility with a wary audience that overwhelmingly demands it.  [34]      Prioritize Verification Over Speed:  Generative AI should be treated as a powerful but unreliable assistant. It can be used for brainstorming visual ideas or creating initial drafts, but all outputs must be subjected to rigorous human fact-checking and verification before publication. An AI-generated visual should never be used to represent factual data (e.g., a chart, a map, a medical diagram) without confirming its absolute accuracy against the original source data. Use AI to Visualize Uncertainty:  Rather than using AI solely to create polished, definitive-looking images that may inadvertently overstate scientific certainty, communicators should explore its potential to visualize uncertainty itself. For example, AI could be prompted to generate a range of possible outcomes based on a probabilistic model, helping to educate the public about concepts like confidence intervals and the nature of scientific forecasting. This turns a potential weakness of AI (variability) into a communicative strength. For Technology Companies Integrate Robust Provenance Tools:  The most effective way to ensure transparency is to build it into the tools themselves. Technology companies should integrate robust content credentialing features directly into their generative AI products. This would create an unremovable digital \"nutrition label\" for every piece of synthetic media, detailing its origin, the tool used to create it, and its subsequent modification history, providing a verifiable chain of custody.  [14]      Develop Sector-Specific Models:  To mitigate the risk of inaccuracies and \"hallucinations\" in critical domains, companies should invest in developing and fine-tuning models specifically for scientific and medical applications. These models should be trained on vetted, high-quality, and ethically sourced datasets from the relevant fields to significantly reduce the rate of error and bias compared to general-purpose models.  [15]      Offer \"Hallucination Insurance\" and Accountability Frameworks:  As AI becomes more integrated into professional workflows, the risk of harm from inaccurate outputs increases. Tech companies, perhaps in partnership with the insurance industry, should explore frameworks for accountability. This could include offering forms of \"hallucination insurance\" that provide recourse for individuals or organizations who suffer tangible harm due to factually incorrect AI outputs, thereby creating a financial incentive for companies to maximize the accuracy and reliability of their systems.  [68]",
      "entry_type": "assistant",
      "deep_research_sources": "Sources used in the report[jcom.sissa.itAll Eyez on AI: A Roadmap for Science Communication Research in the Age of Artificial IntelligenceOpens in a new window](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)[jcom.sissa.itScience communication in the age of artificial intelligenceOpens in a new window](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)[byteplus.comCase study images for artificial intelligence: A comprehensive guide - BytePlusOpens in a new window](https://www.byteplus.com/en/topic/411228)[elm.umaryland.eduScience Communication in the Age of GenAI: Between Trust, Truth, and Transformation - The Elm - The University of Maryland, BaltimoreOpens in a new window](https://elm.umaryland.edu/elm-stories/2025/Science-Communication-in-the-Age-of-GenAI-Between-Trust-Truth-and-Transformation.php)[turacoz.comThe Power of Visualization: How Scientific Diagrams, Graphs, Videos, Infographics Boosts Publications and Citations - Turacoz Healthcare SolutionsOpens in a new window](https://turacoz.com/the-power-of-visualization-how-scientific-diagrams-graphs-videos-infographics-boosts-publications-and-citations/)[misinforeview.hks.harvard.eduThe origin of public concerns over AI supercharging misinformation in the 2024 U.S. presidential electionOpens in a new window](https://misinforeview.hks.harvard.edu/article/the-origin-of-public-concerns-over-ai-supercharging-misinformation-in-the-2024-u-s-presidential-election/)[en.wikipedia.orgAI slop - WikipediaOpens in a new window](https://en.wikipedia.org/wiki/AI_slop)[cbsnews.comDeepfake videos impersonating real doctors push false medical ...Opens in a new window](https://www.cbsnews.com/news/deepfake-videos-impersonating-real-doctors-push-false-medical-advice-treatments/)[cloud.google.comReal-world gen AI use cases from the world's leading organizations | Google Cloud BlogOpens in a new window](https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders)[digitaldefynd.com25 Generative AI Case Studies [In Depth][2025] - DigitalDefyndOpens in a new window](https://digitaldefynd.com/IQ/generative-ai-case-studies/)[en.wikipedia.orgArtificial intelligence visual art - WikipediaOpens in a new window](https://en.wikipedia.org/wiki/Artificial_intelligence_visual_art)[hai.stanford.eduThe 2025 AI Index Report | Stanford HAIOpens in a new window](https://hai.stanford.edu/ai-index/2025-ai-index-report)[hai.stanford.eduArtificial Intelligence Index Report 2025 | Stanford HAIOpens in a new window](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)[simplifiedsciencepublishing.comBest AI Image Generators for Scientists - Simplified Science PublishingOpens in a new window](https://www.simplifiedsciencepublishing.com/resources/best-artificial-intelligence-image-generators-for-scientists)[researchgate.netSystematic Evaluation of AI-Based Text-to-Image Models for ...Opens in a new window](https://www.researchgate.net/publication/393344569_Systematic_Evaluation_of_AI-Based_Text-to-Image_Models_for_Generating_Medical_Illustrations_in_Neurosurgery_A_Multi-Stage_Comparative_Study)[jcom.sissa.itVolume 24 • Issue 02 • 2025 • Science Communication in the Age of ...Opens in a new window](https://jcom.sissa.it/collections/129/)[hai.stanford.eduPublic Opinion | The 2025 AI Index Report - Stanford HAIOpens in a new window](https://hai.stanford.edu/ai-index/2025-ai-index-report/public-opinion)[europa.euEuropean citizens' knowledge and attitudes towards science and technology - February 2025 - - Eurobarometer surveyOpens in a new window](https://europa.eu/eurobarometer/surveys/detail/3227)[pewresearch.orgArtificial Intelligence - Research and data from Pew Research CenterOpens in a new window](https://www.pewresearch.org/topic/internet-technology/emerging-technology/artificial-intelligence/)[pewresearch.orgWhat the data says about Americans' views of artificial intelligence - Pew Research CenterOpens in a new window](https://www.pewresearch.org/short-reads/2023/11/21/what-the-data-says-about-americans-views-of-artificial-intelligence/)[odsc.medium.comAmericans and AI Experts Split on Artificial Intelligence's Future Impact | by ODSCOpens in a new window](https://odsc.medium.com/americans-and-ai-experts-split-on-artificial-intelligences-future-impact-7ee930e1c857)[pewresearch.orgPew Research Center, April 2025, “How the US Public and AI ...Opens in a new window](https://www.pewresearch.org/wp-content/uploads/sites/20/2025/04/pi_2025.04.03_us-public-and-ai-experts_report.pdf)[whitecase.comWhite House Unveils Comprehensive AI Strategy: “Winning the ...Opens in a new window](https://www.whitecase.com/insight-alert/white-house-unveils-comprehensive-ai-strategy-winning-race-americas-ai-action-plan)[artificialintelligenceact.euArticle 50: Transparency Obligations for Providers and Deployers of Certain AI Systems | EU Artificial Intelligence ActOpens in a new window](https://artificialintelligenceact.eu/article/50/)[chinalegalexperts.comChina Deep Synthesis Regulation 2025: Essential GuideOpens in a new window](https://www.chinalegalexperts.com/news/china-deep-synthesis-regulation)[insideprivacy.comChina Releases New Labeling Requirements for AI-Generated Content - Inside PrivacyOpens in a new window](https://www.insideprivacy.com/international/china/china-releases-new-labeling-requirements-for-ai-generated-content/)[transparency.meta.comMisinformation - Transparency CenterOpens in a new window](https://transparency.meta.com/policies/community-standards/misinformation)[health.ec.europa.euArtificial Intelligence in healthcare - Public Health - European CommissionOpens in a new window](https://health.ec.europa.eu/ehealth-digital-health-and-care/artificial-intelligence-healthcare_en)[bioid.comEU AI Act 2024 | Regulations and Handling of Deepfakes - BioIDOpens in a new window](https://www.bioid.com/2024/06/03/eu-ai-act-deepfake-regulations/)[whitehouse.govAmerica's AI Action Plan - The White HouseOpens in a new window](https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf)[digital-strategy.ec.europa.euEuropean approach to artificial intelligence | Shaping Europe's digital futureOpens in a new window](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence)[cambridge.orgNavigating China's regulatory approach to generative artificial intelligence and large language models | Cambridge Forum on AI: Law and GovernanceOpens in a new window](https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/navigating-chinas-regulatory-approach-to-generative-artificial-intelligence-and-large-language-models/969B2055997BF42DE693B7A1A1B4E8BA)[cnet.comAmericans See Lots of Reasons to Worry About AI, Pew Survey Finds - CNETOpens in a new window](https://www.cnet.com/tech/services-and-software/americans-see-lots-of-reasons-to-worry-about-ai-pew-survey-finds/)[poynter.orgAudiences are still skeptical about generative AI in the news - PoynterOpens in a new window](https://www.poynter.org/ethics-trust/2025/news-audience-feelings-artificial-intelligence-data/)[ldi.upenn.eduAI Breakthrough May Transform Public Health Campaigns - Penn LDIOpens in a new window](https://ldi.upenn.edu/our-work/research-updates/ai-breakthrough-may-transform-public-health-campaigns/)[cdc.govCDC.gov & Generative Artificial Intelligence | Other | CDCOpens in a new window](https://www.cdc.gov/other/generative-ai.html)[2021-2025.state.govArtificial Intelligence (AI) - United States Department of StateOpens in a new window](https://2021-2025.state.gov/artificial-intelligence/)[adaptcentre.ieExplore AI · ADAPT, the SFI Research Centre for AI-Driven Digital Content TechnologyOpens in a new window](https://www.adaptcentre.ie/explore-ai/)[metalmagazine.euThe World Through AI - Metal MagazineOpens in a new window](https://metalmagazine.eu/en/post/the-world-through-ai)[tefhealth.euHome - TEF-Health - Testing and Experimentation Facility for Health and RoboticsOpens in a new window](https://tefhealth.eu/)[arxiv.orgWhat constitutes a Deep Fake? The blurry line between legitimate processing and manipulation under the EU AI Act - arXivOpens in a new window](https://arxiv.org/html/2412.09961v2)[realitydefender.comEU AI Act Compliance: How Organizations Can Prepare for Deepfake Detection Requirements - Reality DefenderOpens in a new window](https://www.realitydefender.com/insights/eu-ai-act-compliance-how-organizations-can-prepare-for-deepfake-detection-require)[weforum.orgWhy China's AI breakthroughs should come as no surprise | World Economic ForumOpens in a new window](https://www.weforum.org/stories/2025/06/china-ai-breakthroughs-no-surprise/)[aoto.comShenzhen Opens Futuristic Science Museum Powered by AOTOOpens in a new window](https://www.aoto.com/news-center/news/dynamic/szsciencemuseum.html)[eyeshenzhen.comScience & Technology Museum, a new landmark in Guangming - EYESHENZHENOpens in a new window](https://www.eyeshenzhen.com/content/2025-05/14/content_31567292.htm)[sciencebusiness.netChina leads EU and US on using artificial intelligence in scienceOpens in a new window](https://sciencebusiness.net/news/ai/china-leads-eu-and-us-using-artificial-intelligence-science)[builtin.comBaidu's ERNIE X1 and ERNIE 4.5 Models Explained | Built InOpens in a new window](https://builtin.com/artificial-intelligence/baidu-ernie-x1-ernie-4-5)[research.baidu.comBlog - Baidu ResearchOpens in a new window](https://research.baidu.com/blog)[botpenguin.comErnie Bot: What it is and How to Use Baidu's AI Chatbot ToolOpens in a new window](https://botpenguin.com/blogs/how-to-use-baidu-ernie-bot)[chinadaily.com.cnChina set to clean up online health content - Chinadaily.com.cnOpens in a new window](https://www.chinadaily.com.cn/a/202508/04/WS688ff13aa310c0209d01abff.html)[lw.comChina's New AI Regulations - Latham & Watkins LLPOpens in a new window](https://www.lw.com/admin/upload/SiteAttachments/Chinas-New-AI-Regulations.pdf)[loc.govChina: Provisions on Deep Synthesis Technology Enter into Effect - The Library of CongressOpens in a new window](https://www.loc.gov/item/global-legal-monitor/2023-04-25/china-provisions-on-deep-synthesis-technology-enter-into-effect/)[scipubplus.comAI-Generated Images in Academic Papers: Current Policies and Best Practices - SciPub+Opens in a new window](https://scipubplus.com/hub/blog/ai-generated-images-in-academic-papers/)[elsevier.comGenerative AI policies for journals - ElsevierOpens in a new window](https://www.elsevier.com/about/policies-and-standards/generative-ai-policies-for-journals)[publishingsupport.iopscience.iop.orgGenerative AI Tools - IOPscience - Publishing SupportOpens in a new window](https://publishingsupport.iopscience.iop.org/questions/generative-ai-tools/)[springernature.comEditorial policies - Springer NatureOpens in a new window](https://www.springernature.com/gp/policies/editorial-policies)[thegaze.mediaThe Nature journal will not publish articles featuring AI-generated photos and videosOpens in a new window](https://thegaze.media/news/the-nature-journal-will-not-publish-articles-featuring-ai-generated-photos-and-videos)[techtimes.comNature Journal Bans AI-Generated Work in Published Works, Talks About Integrity and Ethics - Tech TimesOpens in a new window](https://www.techtimes.com/articles/292522/20230612/nature-journal-bans-ai-generated-work-published-works-talks-integrity.htm)[mitcommlab.mit.eduUsing Generative AI for Your Scientific Writing? Be Aware of Journal PoliciesOpens in a new window](https://mitcommlab.mit.edu/cee/2023/08/27/using-generative-ai-for-your-scientific-writing-be-aware-of-journal-policies/)[researcher-resources.acs.orgArtificial Intelligence (AI) Best Practices and Policies at ACS PublicationsOpens in a new window](https://researcher-resources.acs.org/publish/aipolicy)[policies.google.comGenerative AI Prohibited Use PolicyOpens in a new window](https://policies.google.com/terms/generative-ai/use-policy)[developers.google.comGoogle Search's guidance about AI-generated contentOpens in a new window](https://developers.google.com/search/blog/2023/02/google-search-and-ai-content)[tencentcloud.comIn video review, how to distinguish the violation risks of user-generated content (UGC) and AI-generated content? - Tencent CloudOpens in a new window](https://www.tencentcloud.com/techpedia/121523)[academic.oup.comFrom complexity to clarity: How AI enhances perceptions of scientists and the public's understanding of science | PNAS Nexus | Oxford AcademicOpens in a new window](https://academic.oup.com/pnasnexus/article/3/9/pgae387/7750129)[jcom.sissa.itBalancing realism and trust: AI avatars In science communicationOpens in a new window](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_A03/)[news.mit.eduExplained: Generative AI's environmental impact | MIT NewsOpens in a new window](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117)[unep.orgAI has an environmental problem. Here's what the world can do about that. - UNEPOpens in a new window](https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about)[ibm.comThe future of AI: trends shaping the next 10 years - IBMOpens in a new window](https://www.ibm.com/think/insights/artificial-intelligence-future)[imaginingthedigitalfuture.orgExperts Imagine the Impact of Artificial Intelligence by 2040Opens in a new window](https://imaginingthedigitalfuture.org/wp-content/uploads/2024/02/AI2040-FINAL-White-Paper-2-2.29.24.pdf)[ourworldindata.orgAI timelines: What do experts in artificial intelligence expect for the future?Opens in a new window](https://ourworldindata.org/ai-timelines)[royalsociety.orgScience in the age of AI - Royal SocietyOpens in a new window](https://royalsociety.org/-/media/policy/projects/science-in-the-age-of-ai/science-in-the-age-of-ai-report.pdf)Sources read but not used in the report[mckinsey.comThe state of AI: How organizations are rewiring to capture value - McKinseyOpens in a new window](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)[royalsociety.org2024 US-UK Scientific Forum on science in the age of AI | Royal SocietyOpens in a new window](https://royalsociety.org/news-resources/projects/us-uk-forum-2024/)[rsc-src.caUK/Canada Frontiers of Science: Artificial Intelligence ReportOpens in a new window](https://rsc-src.ca/en/voices/ukcanada-frontiers-science-artificial-intelligence-report)[royalsociety.orgAI and data | Royal SocietyOpens in a new window](https://royalsociety.org/current-topics/ai-data/)[pewresearch.orgAI risks, opportunities, regulation: Views of US public and AI experts | Pew Research CenterOpens in a new window](https://www.pewresearch.org/internet/2025/04/03/views-of-risks-opportunities-and-regulation-of-ai/)[digitaldefynd.comTop 50 AI Scandals [2025] - DigitalDefyndOpens in a new window](https://digitaldefynd.com/IQ/top-ai-scandals/)[interpret.csis.orgChina Academy of Information and Communications Technology (CAICT) ArchivesOpens in a new window](https://interpret.csis.org/original_source/china-academy-of-information-and-communications-technology-caict/)[carnegieendowment.orgHow Some of China's Top AI Thinkers Built Their Own AI Safety InstituteOpens in a new window](https://carnegieendowment.org/research/2025/06/how-some-of-chinas-top-ai-thinkers-built-their-own-ai-safety-institute?lang=en)[reutersinstitute.politics.ox.ac.ukreutersinstitute.politics.ox.ac.ukOpens in a new window](https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2024/public-attitudes-towards-use-ai-and-journalism#:~:text=Early%20experimental%20research%20suggests%20that,than%20that%20created%20by%20humans.&text=This%20tension%20means%20that%20news,and%20how%20to%20communicate%20it.)[commondreams.orgParents' Group Says New Revelations About Meta's AI Chatbot 'Confirm Our Worst Fears'Opens in a new window](https://www.commondreams.org/news/meta-chatbot)[academic.oup.comArtificial intelligence-generated podcasts open new doors to make science accessible: a mixed-method evaluation of quality and the potential for science communication - Oxford AcademicOpens in a new window](https://academic.oup.com/eurjcn/advance-article/doi/10.1093/eurjcn/zvaf074/8161719)[research.aimultiple.com23 Healthcare AI Use Cases with Examples in 2025 - Research AIMultipleOpens in a new window](https://research.aimultiple.com/healthcare-ai-use-cases/)[tencent.comArtificial Intelligence - Business - Tencent 腾讯Opens in a new window](https://www.tencent.com/en-us/business/artificial-intelligence.html)[jckonline.comHow to Benefit From, but Not Get Fooled by, AI - JCK OnlineOpens in a new window](https://www.jckonline.com/editorial-article/ai-social-media-updates/)[gao.govArtificial Intelligence: Generative AI Use and Management at Federal Agencies | U.S. GAOOpens in a new window](https://www.gao.gov/products/gao-25-107653)[cni.orgUK Royal Society Report on Science and AI - Coalition for Networked InformationOpens in a new window](https://www.cni.org/news/uk-royal-society-report-on-science-and-ai)[tech.coAI Gone Wrong: AI Hallucinations & Errors in 2025 - Tech.coOpens in a new window](https://tech.co/news/list-ai-failures-mistakes-errors)[mckinsey.comPublic health's inflection point with generative AI - McKinseyOpens in a new window](https://www.mckinsey.com/industries/social-sector/our-insights/public-healths-inflection-point-with-generative-ai)[imaginovation.net10 Real-World Use Cases of Generative AI in Healthcare - ImaginovationOpens in a new window](https://imaginovation.net/blog/use-cases-examples-generative-ai-healthcare/)[europarl.europa.euArtificial intelligence in the context of cultural heritage and museums - European ParliamentOpens in a new window](https://www.europarl.europa.eu/RegData/etudes/BRIE/2023/747120/EPRS_BRI(2023)747120_EN.pdf)[iris.who.intARTIFICIAL INTELLIGENCE IN PUBLIC HEALTH: LESSONS FROM THE EUROPEAN PUBLIC HEALTH CONFERENCE - IRISOpens in a new window](https://iris.who.int/bitstream/handle/10665/380645/Eurohealth-30-3-13-18-eng.pdf?sequence=1)[who.intHarnessing artificial intelligence for health - World Health Organization (WHO)Opens in a new window](https://www.who.int/teams/digital-health-and-innovation/harnessing-artificial-intelligence-for-health)[blogs.otago.ac.nzDeep Fake: The Ingenuity and the Implications - Blogging: University of OtagoOpens in a new window](https://blogs.otago.ac.nz/ouassa/deep/)[acigjournal.comDeepfake Influence Tactics through the Lens of Cialdini's Principles: Case Studies and the DEEP FRAME Tool Proposal - Applied Cybersecurity & Internet GovernanceOpens in a new window](https://www.acigjournal.com/Deepfake-Influence-Tactics-through-the-Lens-of-Cialdini-s-Principles-Case-Studies,201147,0,2.html)[shutterstock.com327+ Thousand Ai Science Royalty-Free Images, Stock Photos & Pictures | ShutterstockOpens in a new window](https://www.shutterstock.com/search/ai-science)[mdpi.comGenerative Artificial Intelligence and the Evolving Challenge of Deepfake Detection: A Systematic Analysis - MDPIOpens in a new window](https://www.mdpi.com/2224-2708/14/1/17)[dhs.govIncreasing Threat of DeepFake Identities - Homeland SecurityOpens in a new window](https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf)[youtube.comAI accounts impersonating doctors on social media - YouTubeOpens in a new window](https://www.youtube.com/watch?v=iNNA-66vKPE)[cstm.cdstm.cnChina Science and Technology Museum(CSTM) is the only comprehensive museum of science and technology at national level in China. The development of CSTM has undergone three stages and in two different locations. With its foundation stone being laid on November 21st 1984, the first phase construction of the museum kicked off in its old location (No. 1 Bei San Huan Zhong Lu, Xicheng District, Beijing 100029) and was completed for official opening on September 22nd 1988. The second phase construction of the museum started in the same location on February 24, 1998 and was inaugurated on April 29, 2000. The corner stone for the construction of the new CSTM in its current location (No. 5 Beichen East Road, Chaoyang District, Beijing 100012) was laid on May 9, 2006, and the new CSTM was officially opened to the public on September 16, 2009.Opens in a new window](https://cstm.cdstm.cn/AboutUs/index.html)[ndtv.comExperts Warn Of Scammers Using 'Deepfakes' Of Famous Doctors On Social Media - NDTVOpens in a new window](https://www.ndtv.com/world-news/experts-warn-of-scammers-using-deepfakes-of-famous-doctors-on-social-media-6563867)[frontline.thehindu.comFact check: AI doctors on social media spreading fake claims - Frontline - The HinduOpens in a new window](https://frontline.thehindu.com/news/fact-check-ai-doctors-on-social-media-spreading-fake-claims/article67402790.ece)[youtube.comWork for Humankind China - AI-enhanced science and technology museum - YouTubeOpens in a new window](https://www.youtube.com/watch?v=H2yOUii78Io)[miamitodaynews.comFrost Science Museum showcases European AI exhibit - Miami TodayOpens in a new window](https://www.miamitodaynews.com/2025/07/16/frost-science-museum-showcases-european-ai-exhibit/)[mila.quebecThis Climate Does Not Exist: Picturing impacts of the climate crisis ...Opens in a new window](https://mila.quebec/en/article/this-climate-does-not-exist-picturing-impacts-of-the-climate-crisis-with-ai-one-address-at)[undp.orgVisualizing the Future: Artificial Intelligence in Climate Action ...Opens in a new window](https://www.undp.org/acceleratorlabs/blog/visualizing-future-artificial-intelligence-climate-action)",
      "thinking_panel": "Research ApproachI'm starting my research by focusing on the impact of AI-generated visuals on public trust in science communication, specifically between January 2024 and August 2025, across the US, EU, and China. My plan is to gather information on case studies, academic research, sentiment metrics, and policy responses. I'll also be looking for comparative analyses between these regions, and ultimately, formulate recommendations and explore future outlooks.Initial Information GatheringI'll begin by broadly searching for case studies and academic research to build a foundational understanding. This will help me identify key events, publications, and organizations. I'll then refine my searches to find more specific information, such as sentiment metrics and policy responses, ensuring I cover all three geographical regions comprehensively.Researching websites[jcom.sissa.itBalancing realism and trust: AI avatars In science communication](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_A03/)[news.mit.eduExplained: Generative AI's environmental impact | MIT News](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117)[mckinsey.comThe state of AI: How organizations are rewiring to capture value - McKinsey](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai)[academic.oup.comFrom complexity to clarity: How AI enhances perceptions of scientists and the public's understanding of science | PNAS Nexus | Oxford Academic](https://academic.oup.com/pnasnexus/article/3/9/pgae387/7750129)[cloud.google.comReal-world gen AI use cases from the world's leading organizations | Google Cloud Blog](https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders)[elm.umaryland.eduScience Communication in the Age of GenAI: Between Trust, Truth, and Transformation - The Elm - The University of Maryland, Baltimore](https://elm.umaryland.edu/elm-stories/2025/Science-Communication-in-the-Age-of-GenAI-Between-Trust-Truth-and-Transformation.php)[jcom.sissa.itVolume 24 • Issue 02 • 2025 • Science Communication in the Age of Artificial Intelligence (Science Communication & AI) - Journal of Science Communication | Collection:](https://jcom.sissa.it/collections/129/)[digitaldefynd.com25 Generative AI Case Studies [In Depth][2025] - DigitalDefynd](https://digitaldefynd.com/IQ/generative-ai-case-studies/)[jcom.sissa.itAll Eyez on AI: A Roadmap for Science Communication Research in the Age of Artificial Intelligence](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)[scipubplus.comAI-Generated Images in Academic Papers: Current Policies and Best Practices - SciPub+](https://scipubplus.com/hub/blog/ai-generated-images-in-academic-papers/)[springernature.comEditorial policies - Springer Nature](https://www.springernature.com/gp/policies/editorial-policies)[thegaze.mediaThe Nature journal will not publish articles featuring AI-generated photos and videos](https://thegaze.media/news/the-nature-journal-will-not-publish-articles-featuring-ai-generated-photos-and-videos)[techtimes.comNature Journal Bans AI-Generated Work in Published Works, Talks About Integrity and Ethics - Tech Times](https://www.techtimes.com/articles/292522/20230612/nature-journal-bans-ai-generated-work-published-works-talks-integrity.htm)[mitcommlab.mit.eduUsing Generative AI for Your Scientific Writing? Be Aware of Journal Policies](https://mitcommlab.mit.edu/cee/2023/08/27/using-generative-ai-for-your-scientific-writing-be-aware-of-journal-policies/)[cni.orgUK Royal Society Report on Science and AI - Coalition for Networked Information](https://www.cni.org/news/uk-royal-society-report-on-science-and-ai)[royalsociety.orgScience in the age of AI - Royal Society](https://royalsociety.org/-/media/policy/projects/science-in-the-age-of-ai/science-in-the-age-of-ai-report.pdf)[royalsociety.org2024 US-UK Scientific Forum on science in the age of AI | Royal Society](https://royalsociety.org/news-resources/projects/us-uk-forum-2024/)[rsc-src.caUK/Canada Frontiers of Science: Artificial Intelligence Report](https://rsc-src.ca/en/voices/ukcanada-frontiers-science-artificial-intelligence-report)[royalsociety.orgAI and data | Royal Society](https://royalsociety.org/current-topics/ai-data/)[odsc.medium.comAmericans and AI Experts Split on Artificial Intelligence's Future Impact | by ODSC](https://odsc.medium.com/americans-and-ai-experts-split-on-artificial-intelligences-future-impact-7ee930e1c857)[pewresearch.orgArtificial Intelligence - Research and data from Pew Research Center](https://www.pewresearch.org/topic/internet-technology/emerging-technology/artificial-intelligence/)[pewresearch.orgPew Research Center, April 2025, “How the US Public and AI Experts View Artificial Intelligence”](https://www.pewresearch.org/wp-content/uploads/sites/20/2025/04/pi_2025.04.03_us-public-and-ai-experts_report.pdf)[cnet.comAmericans See Lots of Reasons to Worry About AI, Pew Survey Finds - CNET](https://www.cnet.com/tech/services-and-software/americans-see-lots-of-reasons-to-worry-about-ai-pew-survey-finds/)[pewresearch.orgAI risks, opportunities, regulation: Views of US public and AI experts | Pew Research Center](https://www.pewresearch.org/internet/2025/04/03/views-of-risks-opportunities-and-regulation-of-ai/)[misinforeview.hks.harvard.eduThe origin of public concerns over AI supercharging misinformation in the 2024 U.S. presidential election](https://misinforeview.hks.harvard.edu/article/the-origin-of-public-concerns-over-ai-supercharging-misinformation-in-the-2024-u-s-presidential-election/)[digitaldefynd.comTop 50 AI Scandals [2025] - DigitalDefynd](https://digitaldefynd.com/IQ/top-ai-scandals/)[jcom.sissa.itScience communication in the age of artificial intelligence](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)[en.wikipedia.orgAI slop - Wikipedia](https://en.wikipedia.org/wiki/AI_slop)[hai.stanford.eduThe 2025 AI Index Report | Stanford HAI](https://hai.stanford.edu/ai-index/2025-ai-index-report)[weforum.orgWhy China's AI breakthroughs should come as no surprise | World Economic Forum](https://www.weforum.org/stories/2025/06/china-ai-breakthroughs-no-surprise/)[europa.euEuropean citizens' knowledge and attitudes towards science and technology - February 2025 - - Eurobarometer survey](https://europa.eu/eurobarometer/surveys/detail/3227)[hai.stanford.eduPublic Opinion | The 2025 AI Index Report - Stanford HAI](https://hai.stanford.edu/ai-index/2025-ai-index-report/public-opinion)[researcher-resources.acs.orgArtificial Intelligence (AI) Best Practices and Policies at ACS Publications](https://researcher-resources.acs.org/publish/aipolicy)[elsevier.comGenerative AI policies for journals - Elsevier](https://www.elsevier.com/about/policies-and-standards/generative-ai-policies-for-journals)[publishingsupport.iopscience.iop.orgGenerative AI Tools - IOPscience - Publishing Support](https://publishingsupport.iopscience.iop.org/questions/generative-ai-tools/)Initial Findings on AI in Science CommunicationI've started by looking for case studies and academic studies related to AI-generated visuals in science communication. I've found that AI is being used to summarize scientific research to make it more accessible, and there's a growing discussion around the term \"AI slop\" for low-quality AI-generated content, with some examples of its misuse in political and marketing contexts. I've also identified a special issue of the *Journal of Science Communication* dedicated to AI, which I plan to explore further.Public Trust and Policy LandscapeMy initial searches have provided valuable insights into public opinion and policy responses. The Pew Research Center has conducted surveys on public and expert views on AI in the US, and the Eurobarometer survey offers data for Europe. The Stanford HAI 2025 AI Index Report also provides high-level data on public opinion and investment trends across the US, EU, and China. Furthermore, I've found that major scientific journals like *Nature* and *Science* have already established policies regarding the use of AI-generated images in publications, which is a direct hit for policy responses from scientific organizations.Next Steps in ResearchMy next step is to delve deeper into the most promising sources I've identified. I'll be browsing the academic journals, the Pew Research Center reports, the Stanford HAI report, and the Royal Society report to extract detailed information on key findings, specific public opinion metrics, and main recommendations. This will help me build a solid foundation for the research and identify any remaining information gaps that require more targeted searches.Researching websites[hai.stanford.eduThe 2025 AI Index Report | Stanford HAI](https://hai.stanford.edu/ai-index/2025-ai-index-report)[pewresearch.orgPew Research Center, April 2025, “How the US Public and AI ...](https://www.pewresearch.org/wp-content/uploads/sites/20/2025/04/pi_2025.04.03_us-public-and-ai-experts_report.pdf)[scipubplus.comAI-Generated Images in Academic Papers: Current Policies and ...](https://scipubplus.com/hub/blog/ai-generated-images-in-academic-papers/)[royalsociety.orgScience in the age of AI - Royal Society](https://royalsociety.org/-/media/policy/projects/science-in-the-age-of-ai/science-in-the-age-of-ai-report.pdf)[jcom.sissa.itVolume 24 • Issue 02 • 2025 • Science Communication in the Age of ...](https://jcom.sissa.it/collections/129/)Synthesizing Initial DiscoveriesI've made good progress in gathering information on AI-generated visuals in science communication. I now have a solid understanding of academic perspectives, highlighting both the potential benefits, such as increased accessibility, and the inherent risks, including misinformation and bias. I've also found quantitative data on public sentiment towards AI in general across the US and EU, with some high-level comparisons for China. A significant finding is the clear policies from major scientific publishers like Nature and Elsevier regarding the use of AI-generated visuals in research, which directly addresses a key aspect of policy responses.Identifying Information GapsWhile I have a good foundation, I've identified several areas where more specific information is needed. I'm still looking for concrete case studies of AI-generated visuals in science communication within the US, EU, and China, particularly those that illustrate public reactions. My current policy information is heavily focused on academic publishers, so I need to expand my search to include governmental bodies and major tech platforms. Additionally, I need more in-depth, specific information on China, beyond general public opinion, and sentiment metrics that specifically address trust in *AI-generated visuals* rather than AI in general.Charting the Path ForwardMy next steps will involve targeted searches to fill these gaps. I'll be looking for specific examples of AI-generated visuals in science communication, investigating policies from governments and tech companies, and focusing on more detailed information for China. I'll also refine my search for sentiment metrics that are specific to AI-generated visuals and seek out expert forecasts on the long-term implications of these visuals for public trust in science.Researching websites[news.mit.eduExplained: Generative AI's environmental impact | MIT News](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117)[jcom.sissa.itScience communication in the age of artificial intelligence](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)[researchgate.netSystematic Evaluation of AI-Based Text-to-Image Models for Generating Medical Illustrations in Neurosurgery: A Multi-Stage Comparative Study | Request PDF - ResearchGate](https://www.researchgate.net/publication/393344569_Systematic_Evaluation_of_AI-Based_Text-to-Image_Models_for_Generating_Medical_Illustrations_in_Neurosurgery_A_Multi-Stage_Comparative_Study)[sciencebusiness.netChina leads EU and US on using artificial intelligence in science](https://sciencebusiness.net/news/ai/china-leads-eu-and-us-using-artificial-intelligence-science)[whitecase.comWhite House Unveils Comprehensive AI Strategy: “Winning the Race: America's AI Action Plan” | White & Case LLP](https://www.whitecase.com/insight-alert/white-house-unveils-comprehensive-ai-strategy-winning-race-americas-ai-action-plan)[hai.stanford.eduArtificial Intelligence Index Report 2025 | Stanford HAI](https://hai.stanford.edu/assets/files/hai_ai_index_report_2025.pdf)[whitehouse.govAmerica's AI Action Plan - The White House](https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf)[2021-2025.state.govArtificial Intelligence (AI) - United States Department of State](https://2021-2025.state.gov/artificial-intelligence/)[artificialintelligenceact.euArticle 50: Transparency Obligations for Providers and Deployers of Certain AI Systems | EU Artificial Intelligence Act](https://artificialintelligenceact.eu/article/50/)[arxiv.orgWhat constitutes a Deep Fake? The blurry line between legitimate processing and manipulation under the EU AI Act - arXiv](https://arxiv.org/html/2412.09961v2)[interpret.csis.orgChina Academy of Information and Communications Technology (CAICT) Archives](https://interpret.csis.org/original_source/china-academy-of-information-and-communications-technology-caict/)[carnegieendowment.orgHow Some of China's Top AI Thinkers Built Their Own AI Safety Institute](https://carnegieendowment.org/research/2025/06/how-some-of-chinas-top-ai-thinkers-built-their-own-ai-safety-institute?lang=en)[bioid.comEU AI Act 2024 | Regulations and Handling of Deepfakes - BioID](https://www.bioid.com/2024/06/03/eu-ai-act-deepfake-regulations/)[poynter.orgAudiences are still skeptical about generative AI in the news - Poynter](https://www.poynter.org/ethics-trust/2025/news-audience-feelings-artificial-intelligence-data/)[ibm.comThe future of AI: trends shaping the next 10 years - IBM](https://www.ibm.com/think/insights/artificial-intelligence-future)[reutersinstitute.politics.ox.ac.ukreutersinstitute.politics.ox.ac.uk](https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2024/public-attitudes-towards-use-ai-and-journalism#:~:text=Early%20experimental%20research%20suggests%20that,than%20that%20created%20by%20humans.&text=This%20tension%20means%20that%20news,and%20how%20to%20communicate%20it.)[ourworldindata.orgAI timelines: What do experts in artificial intelligence expect for the future?](https://ourworldindata.org/ai-timelines)[imaginingthedigitalfuture.orgExperts Imagine the Impact of Artificial Intelligence by 2040](https://imaginingthedigitalfuture.org/wp-content/uploads/2024/02/AI2040-FINAL-White-Paper-2-2.29.24.pdf)[builtin.comBaidu's ERNIE X1 and ERNIE 4.5 Models Explained | Built In](https://builtin.com/artificial-intelligence/baidu-ernie-x1-ernie-4-5)[botpenguin.comErnie Bot: What it is and How to Use Baidu's AI Chatbot Tool](https://botpenguin.com/blogs/how-to-use-baidu-ernie-bot)[research.baidu.comBlog - Baidu Research](https://research.baidu.com/blog)[tencent.comArtificial Intelligence - Business - Tencent 腾讯](https://www.tencent.com/en-us/business/artificial-intelligence.html)[insideprivacy.comChina Releases New Labeling Requirements for AI-Generated Content - Inside Privacy](https://www.insideprivacy.com/international/china/china-releases-new-labeling-requirements-for-ai-generated-content/)[tencentcloud.comIn video review, how to distinguish the violation risks of user-generated content (UGC) and AI-generated content? - Tencent Cloud](https://www.tencentcloud.com/techpedia/121523)[transparency.meta.comMisinformation - Transparency Center](https://transparency.meta.com/policies/community-standards/misinformation)[jckonline.comHow to Benefit From, but Not Get Fooled by, AI - JCK Online](https://www.jckonline.com/editorial-article/ai-social-media-updates/)[commondreams.orgParents' Group Says New Revelations About Meta's AI Chatbot 'Confirm Our Worst Fears'](https://www.commondreams.org/news/meta-chatbot)[policies.google.comGenerative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy)[developers.google.comGoogle Search's guidance about AI-generated content](https://developers.google.com/search/blog/2023/02/google-search-and-ai-content)[academic.oup.comArtificial intelligence-generated podcasts open new doors to make science accessible: a mixed-method evaluation of quality and the potential for science communication - Oxford Academic](https://academic.oup.com/eurjcn/advance-article/doi/10.1093/eurjcn/zvaf074/8161719)[chinalegalexperts.comChina Deep Synthesis Regulation 2025: Essential Guide](https://www.chinalegalexperts.com/news/china-deep-synthesis-regulation)[lw.comChina's New AI Regulations - Latham & Watkins LLP](https://www.lw.com/admin/upload/SiteAttachments/Chinas-New-AI-Regulations.pdf)[loc.govChina: Provisions on Deep Synthesis Technology Enter into Effect - The Library of Congress](https://www.loc.gov/item/global-legal-monitor/2023-04-25/china-provisions-on-deep-synthesis-technology-enter-into-effect/)Refining Understanding of AI's ImpactI've been synthesizing the information gathered so far, and I'm gaining a clearer picture of how AI-generated visuals are influencing science communication. I've noted that while AI can enhance accessibility and comprehension of scientific research, there are significant concerns about its potential to increase misinformation and bias. Interestingly, a 2024 study found that AI-generated summaries of scientific research were perceived as more trustworthy and led to better recall of information by lay readers. However, there's also a growing awareness of 'AI slop' – low-quality, high-volume AI-generated content – which has led to controversies in various sectors, including political campaigns and marketing, and raises concerns about factual accuracy and data privacy.Addressing Information Gaps and Regional NuancesWhile I have a good overview of public opinion on AI in general across the US, EU, and China, I'm still working to pinpoint specific case studies of AI-generated visuals in science communication and their direct impact on public trust within these regions. For instance, the Stanford HAI 2025 AI Index Report indicates rising global AI optimism, but with deep regional divides, showing higher optimism in China compared to the US and Europe. The Eurobarometer survey also reveals mixed views in Europe regarding trust in scientific research created with AI. I'm also looking for more granular data on how these general sentiments translate specifically to trust in AI-generated *visuals* in a scientific context. Furthermore, I'm seeking more detailed insights into China's specific responses and public reactions to AI-generated content in science communication.Strategic Next StepsMy next steps will involve a series of targeted explorations of the existing sources to extract more specific details on policy frameworks and any mentioned case studies. Following that, I will conduct new, more precise searches to uncover concrete examples of AI-generated visuals in science communication, focusing on news reports, academic analyses, and reports from relevant organizations. I'll also prioritize finding English-language sources from China to gain a deeper understanding of their unique context and look for information on AI's use in science museums and educational settings, as these are likely areas for visual AI application.Researching websites[transparency.meta.comMisinformation | Transparency Center](https://transparency.meta.com/policies/community-standards/misinformation)[builtin.comBaidu's ERNIE X1 and ERNIE 4.5 Models Explained | Built In](https://builtin.com/artificial-intelligence/baidu-ernie-x1-ernie-4-5)[whitecase.comWhite House Unveils Comprehensive AI Strategy: “Winning the ...](https://www.whitecase.com/insight-alert/white-house-unveils-comprehensive-ai-strategy-winning-race-americas-ai-action-plan)[poynter.orgAudiences are still skeptical about generative AI in the news - Poynter](https://www.poynter.org/ethics-trust/2025/news-audience-feelings-artificial-intelligence-data/)[chinalegalexperts.comChina Deep Synthesis Regulation 2025: Essential Guide](https://www.chinalegalexperts.com/news/china-deep-synthesis-regulation)[researchgate.netSystematic Evaluation of AI-Based Text-to-Image Models for ...](https://www.researchgate.net/publication/393344569_Systematic_Evaluation_of_AI-Based_Text-to-Image_Models_for_Generating_Medical_Illustrations_in_Neurosurgery_A_Multi-Stage_Comparative_Study)[artificialintelligenceact.euArticle 50: Transparency Obligations for Providers and Deployers of ...](https://artificialintelligenceact.eu/article/50/)Progress on Policy and Public SentimentI've made significant strides in understanding the policy landscape and public opinion surrounding AI-generated visuals in science communication. I now have a solid grasp of the key regulatory approaches in the US (AI Action Plan), EU (AI Act), and China (Deep Synthesis Regulation), noting their differing emphases on risk, transparency, and content control. Quantitative data from sources like Pew, Eurobarometer, and Stanford HAI consistently show higher AI optimism in China compared to the US and EU, alongside a general public skepticism towards AI in news and a strong desire for disclosure. The scientific community's stance is also clearer, with major publishers requiring disclosure and documentation for AI-generated visuals.Addressing Case Study GapsWhile I have a good theoretical understanding, a critical gap remains in finding specific, detailed case studies, particularly for the EU and China. Without these concrete examples, my analysis of real-world impacts will be too abstract. My current insights on case studies are limited, with some general examples of AI use in marketing and politics, but not enough specific instances within science communication itself. I need to move beyond general trends and identify how AI-generated visuals have been specifically used in science communication and the public's reaction to them.Next Steps for Deeper InsightsMy immediate focus is to unearth concrete case studies. I will conduct highly targeted searches for examples of AI-generated visuals in science communication, looking for reports from science journalism, university press offices, and NGOs, and specifically seeking out controversies or debates. I will also deepen my research into China and the EU by looking for English-language articles from their respective media outlets and research institutions to find specific instances of generative AI use in science outreach and education. Finally, I will conduct a targeted search for academic studies or market research that specifically measure public trust in AI-generated *images* and *videos* within an informational or scientific context. This focused approach will allow me to synthesize a comprehensive and well-supported report.Researching websites[en.wikipedia.orgArtificial intelligence visual art - Wikipedia](https://en.wikipedia.org/wiki/Artificial_intelligence_visual_art)[sciencebusiness.netChina leads EU and US on using artificial intelligence in science](https://sciencebusiness.net/news/ai/china-leads-eu-and-us-using-artificial-intelligence-science)[jcom.sissa.itBalancing realism and trust: AI avatars In science communication](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_A03/)[byteplus.comCase study images for artificial intelligence: A comprehensive guide - BytePlus](https://www.byteplus.com/en/topic/411228)[jcom.sissa.itScience communication in the age of artificial intelligence](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_E/)[research.aimultiple.com23 Healthcare AI Use Cases with Examples in 2025 - Research AIMultiple](https://research.aimultiple.com/healthcare-ai-use-cases/)[turacoz.comThe Power of Visualization: How Scientific Diagrams, Graphs, Videos, Infographics Boosts Publications and Citations - Turacoz Healthcare Solutions](https://turacoz.com/the-power-of-visualization-how-scientific-diagrams-graphs-videos-infographics-boosts-publications-and-citations/)[ibm.comThe future of AI: trends shaping the next 10 years - IBM](https://www.ibm.com/think/insights/artificial-intelligence-future)[poynter.orgAudiences are still skeptical about generative AI in the news - Poynter](https://www.poynter.org/ethics-trust/2025/news-audience-feelings-artificial-intelligence-data/)[tencent.comArtificial Intelligence - Business - Tencent 腾讯](https://www.tencent.com/en-us/business/artificial-intelligence.html)[builtin.comBaidu's ERNIE X1 and ERNIE 4.5 Models Explained | Built In](https://builtin.com/artificial-intelligence/baidu-ernie-x1-ernie-4-5)[botpenguin.comErnie Bot: What it is and How to Use Baidu's AI Chatbot Tool](https://botpenguin.com/blogs/how-to-use-baidu-ernie-bot)[policies.google.comGenerative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy)[developers.google.comGoogle Search's guidance about AI-generated content](https://developers.google.com/search/blog/2023/02/google-search-and-ai-content)[transparency.meta.comMisinformation - Transparency Center](https://transparency.meta.com/policies/community-standards/misinformation)[artificialintelligenceact.euArticle 50: Transparency Obligations for Providers and Deployers of Certain AI Systems | EU Artificial Intelligence Act](https://artificialintelligenceact.eu/article/50/)[jckonline.comHow to Benefit From, but Not Get Fooled by, AI - JCK Online](https://www.jckonline.com/editorial-article/ai-social-media-updates/)[realitydefender.comEU AI Act Compliance: How Organizations Can Prepare for Deepfake Detection Requirements - Reality Defender](https://www.realitydefender.com/insights/eu-ai-act-compliance-how-organizations-can-prepare-for-deepfake-detection-require)[chinalegalexperts.comChina Deep Synthesis Regulation 2025: Essential Guide](https://www.chinalegalexperts.com/news/china-deep-synthesis-regulation)[cambridge.orgNavigating China's regulatory approach to generative artificial intelligence and large language models | Cambridge Forum on AI: Law and Governance](https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/navigating-chinas-regulatory-approach-to-generative-artificial-intelligence-and-large-language-models/969B2055997BF42DE693B7A1A1B4E8BA)[gao.govArtificial Intelligence: Generative AI Use and Management at Federal Agencies | U.S. GAO](https://www.gao.gov/products/gao-25-107653)[jcom.sissa.itAll Eyez on AI: A Roadmap for Science Communication Research in the Age of Artificial Intelligence](https://jcom.sissa.it/article/pubid/JCOM_2402_2025_Y01/)[researcher-resources.acs.orgArtificial Intelligence (AI) Best Practices and Policies at ACS Publications](https://researcher-resources.acs.org/publish/aipolicy)[scipubplus.comAI-Generated Images in Academic Papers: Current Policies and Best Practices - SciPub+](https://scipubplus.com/hub/blog/ai-generated-images-in-academic-papers/)[royalsociety.orgScience in the age of AI - Royal Society](https://royalsociety.org/-/media/policy/projects/science-in-the-age-of-ai/science-in-the-age-of-ai-report.pdf)[cni.orgUK Royal Society Report on Science and AI - Coalition for Networked Information](https://www.cni.org/news/uk-royal-society-report-on-science-and-ai)[springernature.comEditorial policies - Springer Nature](https://www.springernature.com/gp/policies/editorial-policies)[tech.coAI Gone Wrong: AI Hallucinations & Errors in 2025 - Tech.co](https://tech.co/news/list-ai-failures-mistakes-errors)[pewresearch.orgArtificial Intelligence - Research and data from Pew Research Center](https://www.pewresearch.org/topic/internet-technology/emerging-technology/artificial-intelligence/)[pewresearch.orgWhat the data says about Americans' views of artificial intelligence - Pew Research Center](https://www.pewresearch.org/short-reads/2023/11/21/what-the-data-says-about-americans-views-of-artificial-intelligence/)Synthesizing Current KnowledgeI've been synthesizing the information gathered so far, and I have a strong foundation regarding policy responses, public sentiment, and the scientific community's stance on AI-generated visuals. I've noted that while AI offers potential for enhancing accessibility in science communication, concerns about misinformation and bias are prevalent. Major scientific journals have established clear policies requiring disclosure for AI-generated visuals, and public opinion surveys indicate a desire for greater transparency and regulation. There's also a clear regional divide in AI optimism, with China generally more optimistic than the US and EU.Addressing Remaining Information GapsDespite the progress, I'm still working to identify specific, high-profile case studies of AI-generated visuals being used in science communication and their direct impact on public trust across the US, EU, and China. While I have general examples of AI misuse in other domains, I need more concrete instances within the realm of science communication itself to fully address the prompt. This is a crucial gap that needs to be filled to provide a comprehensive analysis.Targeted Search for Case StudiesMy next step is to conduct a highly targeted search for these specific case studies. I will focus on areas like AI-generated visuals in climate change communication, public health campaigns, and science museums or educational institutions. I'll also be looking for any controversies or viral moments related to AI-generated scientific images that have sparked public debate, specifically seeking examples from the EU and China to ensure a balanced comparative analysis. This focused approach should yield the necessary examples to complete my research.Researching websites[cdc.govCDC.gov & Generative Artificial Intelligence | Other](https://www.cdc.gov/other/generative-ai.html)[mila.quebecThis Climate Does Not Exist: Picturing impacts of the climate crisis with AI, one address at a time | Mila](https://mila.quebec/en/article/this-climate-does-not-exist-picturing-impacts-of-the-climate-crisis-with-ai-one-address-at)[news.mit.eduExplained: Generative AI's environmental impact | MIT News](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117)[unep.orgAI has an environmental problem. Here's what the world can do about that. - UNEP](https://www.unep.org/news-and-stories/story/ai-has-environmental-problem-heres-what-world-can-do-about)[miamitodaynews.comFrost Science Museum showcases European AI exhibit - Miami Today](https://www.miamitodaynews.com/2025/07/16/frost-science-museum-showcases-european-ai-exhibit/)[ldi.upenn.eduAI Breakthrough May Transform Public Health Campaigns - Penn LDI](https://ldi.upenn.edu/our-work/research-updates/ai-breakthrough-may-transform-public-health-campaigns/)[undp.orgVisualizing the Future: Artificial Intelligence in Climate Action](https://www.undp.org/acceleratorlabs/blog/visualizing-future-artificial-intelligence-climate-action)[chinadaily.com.cnChina set to clean up online health content - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202508/04/WS688ff13aa310c0209d01abff.html)[mckinsey.comPublic health's inflection point with generative AI - McKinsey](https://www.mckinsey.com/industries/social-sector/our-insights/public-healths-inflection-point-with-generative-ai)[imaginovation.net10 Real-World Use Cases of Generative AI in Healthcare - Imaginovation](https://imaginovation.net/blog/use-cases-examples-generative-ai-healthcare/)[europarl.europa.euArtificial intelligence in the context of cultural heritage and museums - European Parliament](https://www.europarl.europa.eu/RegData/etudes/BRIE/2023/747120/EPRS_BRI(2023)747120_EN.pdf)[iris.who.intARTIFICIAL INTELLIGENCE IN PUBLIC HEALTH: LESSONS FROM THE EUROPEAN PUBLIC HEALTH CONFERENCE - IRIS](https://iris.who.int/bitstream/handle/10665/380645/Eurohealth-30-3-13-18-eng.pdf?sequence=1)[adaptcentre.ieExplore AI · ADAPT, the SFI Research Centre for AI-Driven Digital Content Technology](https://www.adaptcentre.ie/explore-ai/)[metalmagazine.euThe World Through AI - Metal Magazine](https://metalmagazine.eu/en/post/the-world-through-ai)[tefhealth.euHome - TEF-Health - Testing and Experimentation Facility for Health and Robotics](https://tefhealth.eu/)[digital-strategy.ec.europa.euEuropean approach to artificial intelligence | Shaping Europe's digital future](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence)[who.intHarnessing artificial intelligence for health - World Health Organization (WHO)](https://www.who.int/teams/digital-health-and-innovation/harnessing-artificial-intelligence-for-health)[health.ec.europa.euArtificial Intelligence in healthcare - Public Health - European Commission](https://health.ec.europa.eu/ehealth-digital-health-and-care/artificial-intelligence-healthcare_en)[blogs.otago.ac.nzDeep Fake: The Ingenuity and the Implications - Blogging: University of Otago](https://blogs.otago.ac.nz/ouassa/deep/)[acigjournal.comDeepfake Influence Tactics through the Lens of Cialdini's Principles: Case Studies and the DEEP FRAME Tool Proposal - Applied Cybersecurity & Internet Governance](https://www.acigjournal.com/Deepfake-Influence-Tactics-through-the-Lens-of-Cialdini-s-Principles-Case-Studies,201147,0,2.html)[en.wikipedia.orgAI slop - Wikipedia](https://en.wikipedia.org/wiki/AI_slop)[simplifiedsciencepublishing.comBest AI Image Generators for Scientists - Simplified Science Publishing](https://www.simplifiedsciencepublishing.com/resources/best-artificial-intelligence-image-generators-for-scientists)[en.wikipedia.orgArtificial intelligence visual art - Wikipedia](https://en.wikipedia.org/wiki/Artificial_intelligence_visual_art)[shutterstock.com327+ Thousand Ai Science Royalty-Free Images, Stock Photos & Pictures | Shutterstock](https://www.shutterstock.com/search/ai-science)[mdpi.comGenerative Artificial Intelligence and the Evolving Challenge of Deepfake Detection: A Systematic Analysis - MDPI](https://www.mdpi.com/2224-2708/14/1/17)[dhs.govIncreasing Threat of DeepFake Identities - Homeland Security](https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf)[cbsnews.comDeepfake videos impersonating real doctors push false medical advice and treatments](https://www.cbsnews.com/news/deepfake-videos-impersonating-real-doctors-push-false-medical-advice-treatments/)[youtube.comAI accounts impersonating doctors on social media - YouTube](https://www.youtube.com/watch?v=iNNA-66vKPE)[cstm.cdstm.cnChina Science and Technology Museum(CSTM) is the only comprehensive museum of science and technology at national level in China. The development of CSTM has undergone three stages and in two different locations. With its foundation stone being laid on November 21st 1984, the first phase construction of the museum kicked off in its old location (No. 1 Bei San Huan Zhong Lu, Xicheng District, Beijing 100029) and was completed for official opening on September 22nd 1988. The second phase construction of the museum started in the same location on February 24, 1998 and was inaugurated on April 29, 2000. The corner stone for the construction of the new CSTM in its current location (No. 5 Beichen East Road, Chaoyang District, Beijing 100012) was laid on May 9, 2006, and the new CSTM was officially opened to the public on September 16, 2009.](https://cstm.cdstm.cn/AboutUs/index.html)[ndtv.comExperts Warn Of Scammers Using 'Deepfakes' Of Famous Doctors On Social Media - NDTV](https://www.ndtv.com/world-news/experts-warn-of-scammers-using-deepfakes-of-famous-doctors-on-social-media-6563867)[eyeshenzhen.comScience & Technology Museum, a new landmark in Guangming - EYESHENZHEN](https://www.eyeshenzhen.com/content/2025-05/14/content_31567292.htm)[aoto.comShenzhen Opens Futuristic Science Museum Powered by AOTO](https://www.aoto.com/news-center/news/dynamic/szsciencemuseum.html)[frontline.thehindu.comFact check: AI doctors on social media spreading fake claims - Frontline - The Hindu](https://frontline.thehindu.com/news/fact-check-ai-doctors-on-social-media-spreading-fake-claims/article67402790.ece)[youtube.comWork for Humankind China - AI-enhanced science and technology museum - YouTube](https://www.youtube.com/watch?v=H2yOUii78Io)Emerging Case Studies and Public PerceptionI've identified several emerging case studies that shed light on the use of AI-generated visuals in science communication. For instance, the 'This Climate Does Not Exist' project uses AI to visualize climate change impacts, and there's a growing discussion around AI-generated avatars in science communication, with some research suggesting that higher realism in avatars can enhance trustworthiness, contradicting the 'Uncanny Valley' effect. However, concerns persist about the potential for AI-generated content to be manipulated for misinformation, as seen in political contexts with deepfakes and 'AI slop.'Insights into Trust and Regional DifferencesMy research indicates a complex relationship between AI-generated visuals and public trust. While some studies suggest AI can make scientific information more accessible and even more trustworthy in summary form, there's a clear public concern about AI spreading misinformation and data misuse. Regional differences in AI optimism are also evident, with China showing higher optimism compared to the US and EU. The Eurobarometer survey, for example, shows mixed views in Europe regarding trust in AI-assisted scientific research.Refining Case Study Analysis and Future ActionsWhile I have a good overview of policy and general sentiment, I need to delve deeper into the specific reception and impact of these AI-generated visuals on public trust within the identified case studies. I also need to find more concrete examples from China and the EU to ensure a comprehensive comparative analysis. My next steps will involve using the browse tool to extract more detailed information from existing sources related to these case studies, and conducting targeted searches for additional examples, particularly focusing on how these visuals were perceived by the public and the scientific community in the US, EU, and China.Researching websites[cdc.govCDC.gov & Generative Artificial Intelligence | Other | CDC](https://www.cdc.gov/other/generative-ai.html)[miamitodaynews.comFrost Science Museum showcases European AI exhibit - Miami Today](https://www.miamitodaynews.com/2025/07/16/frost-science-museum-showcases-european-ai-exhibit/)[aoto.comShenzhen Opens Futuristic Science Museum Powered by AOTO](https://www.aoto.com/news-center/news/dynamic/szsciencemuseum.html)[ldi.upenn.eduAI Breakthrough May Transform Public Health Campaigns - Penn LDI](https://ldi.upenn.edu/our-work/research-updates/ai-breakthrough-may-transform-public-health-campaigns/)[mila.quebecThis Climate Does Not Exist: Picturing impacts of the climate crisis ...](https://mila.quebec/en/article/this-climate-does-not-exist-picturing-impacts-of-the-climate-crisis-with-ai-one-address-at)[cbsnews.comDeepfake videos impersonating real doctors push false medical ...](https://www.cbsnews.com/news/deepfake-videos-impersonating-real-doctors-push-false-medical-advice-treatments/)[chinadaily.com.cnChina set to clean up online health content - Chinadaily.com.cn](https://www.chinadaily.com.cn/a/202508/04/WS688ff13aa310c0209d01abff.html)[undp.orgVisualizing the Future: Artificial Intelligence in Climate Action ...](https://www.undp.org/acceleratorlabs/blog/visualizing-future-artificial-intelligence-climate-action)Comprehensive Understanding AchievedI have now completed a thorough research phase, gathering extensive information on the impact of AI-generated visuals on public trust in science communication across the US, EU, and China. I have a strong grasp of the policy landscapes, public opinion, and the practical applications of AI-generated visuals in this domain. I've also identified several case studies that, while not always providing direct metrics on public trust, offer valuable insights into the opportunities and challenges of this technology.Information Saturation and Next StepsI've reached a point of saturation where further general searches are unlikely to yield significant new information. I have a solid foundation to build a detailed report that addresses all aspects of the user's query, including the comparative analysis, recommendations, and future outlook. I have sufficient material to address each of the seven points in the user's request, and I can now move on to the analysis and writing phase.",
      "execution_time_seconds": 602.4561359882355
    }
  ],
  "query_id": 2,
  "topic": "Science & Technology",
  "language": "en"
}